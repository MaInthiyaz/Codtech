#DATA ANALYTICS

*COMPANY*:CODTECH IT SOLUTIONS PVT.LTD

*NAME*:MAHAMMADINTHIYAZ

*INTERN ID*:CT04DA545

*DOMAIN*:DATA ANALYTICS

*DURATION*:4 WEEKS

*MENTOR*:NEELA SANTHOSH KUMAR

#Detailed Description of All 4 Internship Tasks

As part of my *Data Analytics Internship* at *CODTECH IT Solutions*, I completed four comprehensive and practical tasks that helped enhance my skills in handling large datasets, analyzing data, applying machine learning models, and generating insights through data visualization. Each task provided exposure to real-world problems and equipped me with valuable industry-relevant techniques and tools.

# *Task 1: Big Data Analysis*

This task focused on understanding and processing *large-scale datasets* using scalable tools like *PySpark* or *Dask*. I selected a real-world dataset and performed data analysis in a distributed environment to demonstrate how big data can be efficiently handled. The goal was to simulate real-time processing challenges that arise when datasets go beyond the capabilities of traditional tools like Pandas.

Using *PySpark*, I loaded and cleaned the data, handled missing values, filtered relevant fields, and performed grouped aggregations to derive meaningful statistics. For instance, I computed average trip durations based on passenger counts using a New York City taxi trip dataset. This task not only introduced me to PySpark's DataFrame API but also gave me practical experience in writing scalable, memory-efficient code — a critical skill in data engineering and analytics roles.

# *Task 2: Exploratory Data Analysis (EDA)*

Task 2 required performing *Exploratory Data Analysis* on a structured dataset to identify patterns, trends, and correlations. I used Python libraries such as *Pandas, **Matplotlib, and **Seaborn* to conduct univariate and bivariate analyses.

The dataset used (e.g., wine quality or housing prices) was first cleaned and preprocessed. I visualized distributions of numerical features, observed outliers using box plots, and used heatmaps to understand the correlations between variables. EDA is essential to understand the structure of the data and to inform the selection of models in later stages. This task honed my skills in visual storytelling, which is crucial for communicating insights to non-technical stakeholders.

---

# *Task 3: Data Cleaning and Preprocessing*

In this task, I demonstrated my ability to *clean raw, messy data* — a skill every data analyst must master. I worked with a publicly available dataset that had inconsistencies, null values, duplicates, and unstandardized formats.

I applied techniques such as:

* Dropping or imputing missing values
* Removing duplicate records
* Converting data types
* Renaming columns for readability
* Normalizing or scaling numerical fields

After cleaning, I exported a polished dataset ready for machine learning or reporting. This task taught me that without high-quality data, even the most sophisticated models can yield unreliable results.


# Task 4: Predictive Analysis Using Machine Learning*

The final task involved building a *predictive model* using machine learning. I selected a dataset related to either fraud detection or wine quality prediction. After preparing the data, I split it into training and testing sets and applied models like *Linear Regression, **Logistic Regression, or **Random Forest*, depending on whether it was a regression or classification problem.

I evaluated the model using metrics such as:

* Accuracy
* Mean Squared Error (MSE)
* R² score (for regression)
* Precision, Recall, F1-score (for classification)

This task gave me hands-on experience in the full ML pipeline — from data ingestion to model evaluation — using libraries such as *scikit-learn*.








